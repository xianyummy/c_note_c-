{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_classification_with_vision_transformer",
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xianyummy/c_note_c-/blob/main/image_classification_with_vision_transformer_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PX_6DdScf0o"
      },
      "source": [
        "This notebook comes from this tutorial [Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/). I have only modified the hyperparameters to train the underlying model on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsNe8FAPu5zU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0OkBbbbu80d",
        "outputId": "7801dc53-d245-4e30-d875-f8096185cd6a"
      },
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Collecting tensorflow-addons\n",
            "  Using cached tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Using cached tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "Installing collected packages: tensorflow-addons\n",
            "  Attempting uninstall: tensorflow-addons\n",
            "    Found existing installation: tensorflow-addons 0.20.0\n",
            "    Uninstalling tensorflow-addons-0.20.0:\n",
            "      Successfully uninstalled tensorflow-addons-0.20.0\n",
            "Successfully installed tensorflow-addons-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WQLr-Rcu5zV",
        "outputId": "d2657fc1-9563-4da1-a1ab-5e41e4ec3cce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_DrbSiRu5zV"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA2smPSxu5zV",
        "outputId": "b14e2f97-136a-4a22-cbc6-cca1bdf27a5a"
      },
      "source": [
        "num_classes = 100\n",
        "#定義分類問題的類別數量\n",
        "input_shape = (32, 32, 3)\n",
        "#定義輸入數據的形狀 32x32：輸入圖像的寬和高為 32 像素。3：輸入圖像的通道數為 3，表示彩色圖像（RGB）。\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-JvZqfOu5zV"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K902Ap-4u5zV"
      },
      "source": [
        "learning_rate = 0.001\n",
        "#模型訓練時的學習率，控制參數更新的步長。值越大，更新越快，但可能導致訓練不穩定。\n",
        "weight_decay = 0.0001\n",
        "#權重衰減，用於正則化，防止模型過擬合。\n",
        "batch_size = 128\n",
        "#每次訓練中處理的樣本數，較大的批次大小有助於穩定訓練，但需要更多內存。\n",
        "num_epochs = 100\n",
        "#訓練的迭代次數，數據集將被完整掃描 100 次。\n",
        "image_size = 32\n",
        "#輸入圖像的大小為32*32\n",
        "patch_size = 4  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 128\n",
        "#每個 Patch 的特徵會被投影（轉換）成一個 128 維的向量，這是 Transformer 的基本輸入單元。\n",
        "num_heads = 2\n",
        "#使用多頭注意力機制，這裡將每個 Patch 的特徵分成 2 個注意力頭。\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 2\n",
        "mlp_head_units = [projection_dim * 4, projection_dim * 2]  # Size of the dense layers of the final classifier\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUumYGytu5zW"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfsN3KJXu5zW"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.Rescaling(scale=1./255),\n",
        "        layers.experimental.preprocessing.RandomCrop(image_size, image_size),\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\")\n",
        "    ],\n",
        "    name=\"data_augmentation\"\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXwEcxHbu5zW"
      },
      "source": [
        "## Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxMwV3tGu5zW"
      },
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNJGuApRu5zW"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obAurvIfu5zW"
      },
      "source": [
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfFhgVpMu5zX"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "kOHP1MxVu5zX",
        "outputId": "da6305b9-c714-42d0-b0ff-30a9a8e1449e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"original.png\", dpi=300, bbox_inches=\"tight\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n",
        "plt.savefig(\"patched.png\", dpi=300, bbox_inches=\"tight\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 32 X 32\n",
            "Patch size: 4 X 4\n",
            "Patches per image: 64\n",
            "Elements per patch: 48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 64 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jBkglq5u5zX"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
        "vector of size `projection_dim`. In addition, it adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmG-dsffu5zX"
      },
      "source": [
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7TQ-zu4u5zX"
      },
      "source": [
        "## Build the ViT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k43gCKm3u5zY"
      },
      "source": [
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbcoBmQsxHry",
        "outputId": "af1aa42e-8f71-42f0-b5f8-f08b5ffd8f22"
      },
      "source": [
        "create_vit_classifier().count_params()/1e6"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.697572"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZm_hg6uu5zY"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VIbGKt7u5zY",
        "outputId": "b7d058e0-2a40-437c-caf0-74a9341c81ba"
      },
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "352/352 [==============================] - 350s 982ms/step - loss: 2.3680 - accuracy: 0.1902 - top-5-accuracy: 0.7014 - val_loss: 1.8816 - val_accuracy: 0.2806 - val_top-5-accuracy: 0.8348\n",
            "Epoch 2/100\n",
            "352/352 [==============================] - 321s 913ms/step - loss: 1.8695 - accuracy: 0.2991 - top-5-accuracy: 0.8369 - val_loss: 1.6819 - val_accuracy: 0.3834 - val_top-5-accuracy: 0.8824\n",
            "Epoch 3/100\n",
            "352/352 [==============================] - 315s 894ms/step - loss: 1.7050 - accuracy: 0.3729 - top-5-accuracy: 0.8720 - val_loss: 1.4769 - val_accuracy: 0.4638 - val_top-5-accuracy: 0.9234\n",
            "Epoch 4/100\n",
            "352/352 [==============================] - 317s 901ms/step - loss: 1.5741 - accuracy: 0.4259 - top-5-accuracy: 0.8996 - val_loss: 1.3886 - val_accuracy: 0.4992 - val_top-5-accuracy: 0.9364\n",
            "Epoch 5/100\n",
            "352/352 [==============================] - 308s 876ms/step - loss: 1.4895 - accuracy: 0.4628 - top-5-accuracy: 0.9143 - val_loss: 1.3736 - val_accuracy: 0.5284 - val_top-5-accuracy: 0.9264\n",
            "Epoch 6/100\n",
            "352/352 [==============================] - 316s 898ms/step - loss: 1.4151 - accuracy: 0.4907 - top-5-accuracy: 0.9270 - val_loss: 1.2515 - val_accuracy: 0.5662 - val_top-5-accuracy: 0.9544\n",
            "Epoch 7/100\n",
            "352/352 [==============================] - 315s 896ms/step - loss: 1.3503 - accuracy: 0.5189 - top-5-accuracy: 0.9317 - val_loss: 1.1879 - val_accuracy: 0.5828 - val_top-5-accuracy: 0.9586\n",
            "Epoch 8/100\n",
            "352/352 [==============================] - 304s 863ms/step - loss: 1.2856 - accuracy: 0.5443 - top-5-accuracy: 0.9399 - val_loss: 1.1408 - val_accuracy: 0.6050 - val_top-5-accuracy: 0.9534\n",
            "Epoch 9/100\n",
            "352/352 [==============================] - 303s 860ms/step - loss: 1.2268 - accuracy: 0.5638 - top-5-accuracy: 0.9465 - val_loss: 1.0982 - val_accuracy: 0.6078 - val_top-5-accuracy: 0.9594\n",
            "Epoch 10/100\n",
            "352/352 [==============================] - 311s 883ms/step - loss: 1.1737 - accuracy: 0.5905 - top-5-accuracy: 0.9519 - val_loss: 1.0258 - val_accuracy: 0.6372 - val_top-5-accuracy: 0.9674\n",
            "Epoch 11/100\n",
            "352/352 [==============================] - 315s 895ms/step - loss: 1.1367 - accuracy: 0.6008 - top-5-accuracy: 0.9548 - val_loss: 1.0196 - val_accuracy: 0.6342 - val_top-5-accuracy: 0.9706\n",
            "Epoch 12/100\n",
            "352/352 [==============================] - 306s 869ms/step - loss: 1.0854 - accuracy: 0.6188 - top-5-accuracy: 0.9592 - val_loss: 0.9194 - val_accuracy: 0.6738 - val_top-5-accuracy: 0.9746\n",
            "Epoch 13/100\n",
            "352/352 [==============================] - 303s 861ms/step - loss: 1.0348 - accuracy: 0.6392 - top-5-accuracy: 0.9639 - val_loss: 0.9249 - val_accuracy: 0.6714 - val_top-5-accuracy: 0.9746\n",
            "Epoch 14/100\n",
            "352/352 [==============================] - 301s 856ms/step - loss: 0.9964 - accuracy: 0.6515 - top-5-accuracy: 0.9668 - val_loss: 0.9198 - val_accuracy: 0.6848 - val_top-5-accuracy: 0.9694\n",
            "Epoch 15/100\n",
            "352/352 [==============================] - 313s 889ms/step - loss: 0.9645 - accuracy: 0.6597 - top-5-accuracy: 0.9704 - val_loss: 0.8614 - val_accuracy: 0.6992 - val_top-5-accuracy: 0.9726\n",
            "Epoch 16/100\n",
            "352/352 [==============================] - 315s 894ms/step - loss: 0.9214 - accuracy: 0.6776 - top-5-accuracy: 0.9731 - val_loss: 0.8425 - val_accuracy: 0.7058 - val_top-5-accuracy: 0.9796\n",
            "Epoch 17/100\n",
            "352/352 [==============================] - 303s 861ms/step - loss: 0.8913 - accuracy: 0.6892 - top-5-accuracy: 0.9746 - val_loss: 0.8153 - val_accuracy: 0.7210 - val_top-5-accuracy: 0.9814\n",
            "Epoch 18/100\n",
            "352/352 [==============================] - 305s 866ms/step - loss: 0.8544 - accuracy: 0.7008 - top-5-accuracy: 0.9771 - val_loss: 0.8142 - val_accuracy: 0.7216 - val_top-5-accuracy: 0.9802\n",
            "Epoch 19/100\n",
            "352/352 [==============================] - 324s 919ms/step - loss: 0.8300 - accuracy: 0.7088 - top-5-accuracy: 0.9792 - val_loss: 0.7899 - val_accuracy: 0.7218 - val_top-5-accuracy: 0.9800\n",
            "Epoch 20/100\n",
            "352/352 [==============================] - 314s 893ms/step - loss: 0.8107 - accuracy: 0.7154 - top-5-accuracy: 0.9802 - val_loss: 0.7998 - val_accuracy: 0.7190 - val_top-5-accuracy: 0.9772\n",
            "Epoch 21/100\n",
            "352/352 [==============================] - 309s 878ms/step - loss: 0.7766 - accuracy: 0.7264 - top-5-accuracy: 0.9835 - val_loss: 0.7331 - val_accuracy: 0.7486 - val_top-5-accuracy: 0.9820\n",
            "Epoch 22/100\n",
            "340/352 [===========================>..] - ETA: 10s - loss: 0.7636 - accuracy: 0.7325 - top-5-accuracy: 0.9840"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPvTxOirxiPB"
      },
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}